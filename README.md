# Neural_Network_Gradient_Descent
Neural network algorithm with gradient descent is implemented without using a neural network library.


Equation derivations for one hidden layer
Figure 1 Simple neural network architecture with 1 hidden layer

In the backprogation algorithm, first error will be calculated using feed forward.

A multi layer neural network is implemented with using back propagation
algorithm, mini-batch gradient descent and momentum. Different hyperparameters
are experimented to find optimal model. First size of mini batch experimented
when number of hidden units is 15. The best batch size found as 10.
Also while initializing weights, standart deviation of random initialization set
as mô€€€1=2 where m is number of inputs to hidden unit. 


![image](https://user-images.githubusercontent.com/17252665/90631471-7b666280-e22b-11ea-88c9-1e2a16e47e7d.png)
![image](https://user-images.githubusercontent.com/17252665/90631487-828d7080-e22b-11ea-8ce0-a37940cb091e.png)
![image](https://user-images.githubusercontent.com/17252665/90631493-85886100-e22b-11ea-8ee1-48c5d9d8bd09.png)
![image](https://user-images.githubusercontent.com/17252665/90631499-88835180-e22b-11ea-8764-e12c1f7f639f.png)
